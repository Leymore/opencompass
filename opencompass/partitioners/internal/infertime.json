{
    "afqmc-dev": 649.8,
    "agieval-aqua-rat": 132.5,
    "agieval-gaokao-biology": 197.4,
    "agieval-gaokao-chemistry": 241,
    "agieval-gaokao-chinese": 929.7,
    "agieval-gaokao-english": 407.7,
    "agieval-gaokao-geography": 195.5,
    "agieval-gaokao-history": 156.6,
    "agieval-gaokao-mathcloze": 684.8,
    "agieval-gaokao-mathqa": 218.8,
    "agieval-gaokao-physics": 1041.7,
    "agieval-jec-qa-ca": 5709.5,
    "agieval-jec-qa-kd": 5290.5,
    "agieval-logiqa-en": 433.9,
    "agieval-logiqa-zh": 619.9,
    "agieval-lsat-ar": 234.5,
    "agieval-lsat-lr": 505.8,
    "agieval-lsat-rc": 618.6,
    "agieval-math": 6799.5,
    "agieval-sat-en": 551,
    "agieval-sat-en-without-passage": 110.4,
    "agieval-sat-math": 167.1,
    "ARC-c": 113.6,
    "ARC-e": 201,
    "AX_b": 138.2,
    "AX_g": 63.3,
    "bbh-boolean_expressions": 980.9,
    "bbh-causal_judgement": 948.4,
    "bbh-date_understanding": 949.3,
    "bbh-disambiguation_qa": 1272.1,
    "bbh-dyck_languages": 1631,
    "bbh-formal_fallacies": 1967.6,
    "bbh-geometric_shapes": 1438.3,
    "bbh-hyperbaton": 1467,
    "bbh-logical_deduction_five_objects": 1329.2,
    "bbh-logical_deduction_seven_objects": 1540.5,
    "bbh-logical_deduction_three_objects": 1017.4,
    "bbh-movie_recommendation": 1041,
    "bbh-multistep_arithmetic_two": 1324.3,
    "bbh-navigate": 1163.3,
    "bbh-object_counting": 959.7,
    "bbh-penguins_in_a_table": 688.8,
    "bbh-reasoning_about_colored_objects": 1059.3,
    "bbh-ruin_names": 1376.2,
    "bbh-salient_translation_error_detection": 1636.2,
    "bbh-snarks": 803.4,
    "bbh-sports_understanding": 733.2,
    "bbh-temporal_sequences": 1206.9,
    "bbh-tracking_shuffled_objects_five_objects": 1357.2,
    "bbh-tracking_shuffled_objects_seven_objects": 1666.3,
    "bbh-tracking_shuffled_objects_three_objects": 1093.3,
    "bbh-web_of_lies": 1174.4,
    "bbh-word_sorting": 1537.7,
    "BoolQ": 1022.4,
    "bustm-dev": 65.6,
    "bustm-test": 203.3,
    "C3": 1282.8,
    "CB": 73.3,
    "ceval-accountant": 191.5,
    "ceval-advanced_mathematics": 92.4,
    "ceval-art_studies": 97,
    "ceval-basic_medicine": 46.4,
    "ceval-business_administration": 119.3,
    "ceval-chinese_language_and_literature": 94.6,
    "ceval-civil_servant": 212.7,
    "ceval-clinical_medicine": 90,
    "ceval-college_chemistry": 110,
    "ceval-college_economics": 166.1,
    "ceval-college_physics": 91.6,
    "ceval-college_programming": 119,
    "ceval-computer_architecture": 91.5,
    "ceval-computer_network": 80.2,
    "ceval-discrete_mathematics": 76.2,
    "ceval-education_science": 105.6,
    "ceval-electrical_engineer": 127.4,
    "ceval-environmental_impact_assessment_engineer": 121.4,
    "ceval-fire_engineer": 126.3,
    "ceval-high_school_biology": 88.6,
    "ceval-high_school_chemistry": 79.2,
    "ceval-high_school_chinese": 103.6,
    "ceval-high_school_geography": 40.7,
    "ceval-high_school_history": 110.6,
    "ceval-high_school_mathematics": 105.2,
    "ceval-high_school_physics": 80.8,
    "ceval-high_school_politics": 105.3,
    "ceval-ideological_and_moral_cultivation": 54.5,
    "ceval-law": 107.5,
    "ceval-legal_professional": 129.4,
    "ceval-logic": 118,
    "ceval-mao_zedong_thought": 102.9,
    "ceval-marxism": 87.5,
    "ceval-metrology_engineer": 129.1,
    "ceval-middle_school_biology": 98.1,
    "ceval-middle_school_chemistry": 103.5,
    "ceval-middle_school_geography": 70.3,
    "ceval-middle_school_history": 84.9,
    "ceval-middle_school_mathematics": 88.5,
    "ceval-middle_school_physics": 88.2,
    "ceval-middle_school_politics": 112,
    "ceval-modern_chinese_history": 103.4,
    "ceval-operating_system": 51.4,
    "ceval-physician": 140.1,
    "ceval-plant_protection": 73.9,
    "ceval-probability_and_statistics": 95.8,
    "ceval-professional_tour_guide": 103.4,
    "ceval-sports_science": 2190.2,
    "ceval-tax_accountant": 190.2,
    "ceval-teacher_qualification": 126.6,
    "ceval-test-accountant": 1281.4,
    "ceval-test-advanced_mathematics": 484.5,
    "ceval-test-art_studies": 484.7,
    "ceval-test-basic_medicine": 314.3,
    "ceval-test-business_administration": 663.3,
    "ceval-test-chinese_language_and_literature": 379.7,
    "ceval-test-civil_servant": 1498.8,
    "ceval-test-clinical_medicine": 495.4,
    "ceval-test-college_chemistry": 436.5,
    "ceval-test-college_economics": 1002,
    "ceval-test-college_physics": 474.2,
    "ceval-test-college_programming": 727.2,
    "ceval-test-computer_architecture": 458.2,
    "ceval-test-computer_network": 333.2,
    "ceval-test-discrete_mathematics": 284.4,
    "ceval-test-education_science": 532.2,
    "ceval-test-electrical_engineer": 793.2,
    "ceval-test-environmental_impact_assessment_engineer": 3446.6,
    "ceval-test-fire_engineer": 701.9,
    "ceval-test-high_school_biology": 447.9,
    "ceval-test-high_school_chemistry": 394.9,
    "ceval-test-high_school_chinese": 684.2,
    "ceval-test-high_school_geography": 357.1,
    "ceval-test-high_school_history": 447.1,
    "ceval-test-high_school_mathematics": 478.6,
    "ceval-test-high_school_physics": 390.1,
    "ceval-test-high_school_politics": 685.6,
    "ceval-test-ideological_and_moral_cultivation": 308,
    "ceval-test-law": 655.2,
    "ceval-test-legal_professional": 808.2,
    "ceval-test-logic": 812.1,
    "ceval-test-mao_zedong_thought": 555.4,
    "ceval-test-marxism": 379.5,
    "ceval-test-metrology_engineer": 535.4,
    "ceval-test-middle_school_biology": 422,
    "ceval-test-middle_school_chemistry": 473.5,
    "ceval-test-middle_school_geography": 224.6,
    "ceval-test-middle_school_history": 407.9,
    "ceval-test-middle_school_mathematics": 409.7,
    "ceval-test-middle_school_physics": 439.9,
    "ceval-test-middle_school_politics": 589.7,
    "ceval-test-modern_chinese_history": 520.2,
    "ceval-test-operating_system": 289.2,
    "ceval-test-physician": 890.7,
    "ceval-test-plant_protection": 359.3,
    "ceval-test-probability_and_statistics": 561.5,
    "ceval-test-professional_tour_guide": 472.1,
    "ceval-test-sports_science": 314.5,
    "ceval-test-tax_accountant": 1316.3,
    "ceval-test-teacher_qualification": 820,
    "ceval-test-urban_and_rural_planner": 1065.3,
    "ceval-test-veterinary_medicine": 405.1,
    "ceval-urban_and_rural_planner": 164,
    "ceval-veterinary_medicine": 87.1,
    "chid-dev": 186.2,
    "chid-test": 2977.1,
    "cluewsc-dev": 65.3,
    "cluewsc-test": 201,
    "cmmlu-agronomy": 261.8,
    "cmmlu-anatomy": 236.8,
    "cmmlu-ancient_chinese": 310.7,
    "cmmlu-arts": 245.9,
    "cmmlu-astronomy": 271.2,
    "cmmlu-business_ethics": 311.7,
    "cmmlu-chinese_civil_service_exam": 436.5,
    "cmmlu-chinese_driving_rule": 261.4,
    "cmmlu-chinese_food_culture": 237.9,
    "cmmlu-chinese_foreign_policy": 267.5,
    "cmmlu-chinese_history": 712.6,
    "cmmlu-chinese_literature": 326.9,
    "cmmlu-chinese_teacher_qualification": 363,
    "cmmlu-clinical_knowledge": 514,
    "cmmlu-college_actuarial_science": 270.2,
    "cmmlu-college_education": 197.9,
    "cmmlu-college_engineering_hydrology": 210.3,
    "cmmlu-college_law": 230.7,
    "cmmlu-college_mathematics": 259.7,
    "cmmlu-college_medical_statistics": 212.8,
    "cmmlu-college_medicine": 438.9,
    "cmmlu-computer_science": 318.3,
    "cmmlu-computer_security": 329,
    "cmmlu-conceptual_physics": 334.3,
    "cmmlu-construction_project_management": 260.8,
    "cmmlu-economics": 284.1,
    "cmmlu-education": 244.9,
    "cmmlu-electrical_engineering": 294,
    "cmmlu-elementary_chinese": 385.9,
    "cmmlu-elementary_commonsense": 286.9,
    "cmmlu-elementary_information_and_technology": 347,
    "cmmlu-elementary_mathematics": 346.1,
    "cmmlu-ethnology": 224.7,
    "cmmlu-food_science": 229.9,
    "cmmlu-genetics": 298.8,
    "cmmlu-global_facts": 265,
    "cmmlu-high_school_biology": 448.1,
    "cmmlu-high_school_chemistry": 315.5,
    "cmmlu-high_school_geography": 258.8,
    "cmmlu-high_school_mathematics": 272.4,
    "cmmlu-high_school_physics": 280.7,
    "cmmlu-high_school_politics": 391.2,
    "cmmlu-human_sexuality": 212.3,
    "cmmlu-international_law": 324.3,
    "cmmlu-journalism": 262.7,
    "cmmlu-jurisprudence": 688.1,
    "cmmlu-legal_and_moral_basis": 400.2,
    "cmmlu-logical": 234.8,
    "cmmlu-machine_learning": 244.5,
    "cmmlu-management": 320.4,
    "cmmlu-marketing": 303.5,
    "cmmlu-marxist_theory": 326.5,
    "cmmlu-modern_chinese": 244.3,
    "cmmlu-nutrition": 245.8,
    "cmmlu-philosophy": 202.1,
    "cmmlu-professional_accounting": 302.5,
    "cmmlu-professional_law": 449.2,
    "cmmlu-professional_medicine": 557.4,
    "cmmlu-professional_psychology": 366.3,
    "cmmlu-public_relations": 282.1,
    "cmmlu-security_study": 237.9,
    "cmmlu-sociology": 327.8,
    "cmmlu-sports_science": 261.5,
    "cmmlu-traditional_chinese_medicine": 277.5,
    "cmmlu-virology": 291.3,
    "cmmlu-world_history": 434,
    "cmmlu-world_religions": 239.9,
    "cmnli": 4472.1,
    "CMRC_dev": 3798.5,
    "commonsense_qa": 3991.8,
    "COPA": 58,
    "csl_dev": 141.5,
    "csl_test": 1681.1,
    "DRCD_dev": 3924.4,
    "drop": 8036.3,
    "eprstmt-dev": 90.3,
    "eprstmt-test": 177.6,
    "flores_100_afr-eng": 186.1,
    "flores_100_amh-eng": 251.6,
    "flores_100_ara-eng": 251.4,
    "flores_100_asm-eng": 275.5,
    "flores_100_ast-eng": 184.6,
    "flores_100_azj-eng": 234.1,
    "flores_100_bel-eng": 251,
    "flores_100_ben-eng": 261.4,
    "flores_100_bos-eng": 199,
    "flores_100_bul-eng": 222.4,
    "flores_100_cat-eng": 191.6,
    "flores_100_ceb-eng": 197.5,
    "flores_100_ces-eng": 206.8,
    "flores_100_ckb-eng": 254.1,
    "flores_100_cym-eng": 202.1,
    "flores_100_dan-eng": 185.1,
    "flores_100_deu-eng": 174.8,
    "flores_100_ell-eng": 265.7,
    "flores_100_eng-afr": 202.3,
    "flores_100_eng-amh": 314.4,
    "flores_100_eng-ara": 256.2,
    "flores_100_eng-asm": 307.1,
    "flores_100_eng-ast": 189.3,
    "flores_100_eng-azj": 1286.7,
    "flores_100_eng-bel": 242.7,
    "flores_100_eng-ben": 290.2,
    "flores_100_eng-bos": 210.4,
    "flores_100_eng-bul": 212.6,
    "flores_100_eng-cat": 191.6,
    "flores_100_eng-ceb": 218.4,
    "flores_100_eng-ces": 209.5,
    "flores_100_eng-ckb": 295.4,
    "flores_100_eng-cym": 232.3,
    "flores_100_eng-dan": 277.6,
    "flores_100_eng-deu": 199.4,
    "flores_100_eng-ell": 283.8,
    "flores_100_eng-est": 199.6,
    "flores_100_eng-fas": 265,
    "flores_100_eng-fin": 211.7,
    "flores_100_eng-fra": 195.1,
    "flores_100_eng-ful": 228.2,
    "flores_100_eng-gle": 230.5,
    "flores_100_eng-glg": 187.1,
    "flores_100_eng-guj": 304.8,
    "flores_100_eng-hau": 214.8,
    "flores_100_eng-heb": 259.2,
    "flores_100_eng-hin": 277.8,
    "flores_100_eng-hrv": 218.4,
    "flores_100_eng-hun": 213.7,
    "flores_100_eng-hye": 296.2,
    "flores_100_eng-ibo": 238.1,
    "flores_100_eng-ind": 209.2,
    "flores_100_eng-isl": 216.8,
    "flores_100_eng-ita": 199.8,
    "flores_100_eng-jav": 214.9,
    "flores_100_eng-jpn": 221.2,
    "flores_100_eng-kam": 227.3,
    "flores_100_eng-kan": 296.8,
    "flores_100_eng-kat": 287.1,
    "flores_100_eng-kaz": 1279.6,
    "flores_100_eng-kea": 201.4,
    "flores_100_eng-khm": 299.8,
    "flores_100_eng-kir": 1277.3,
    "flores_100_eng-kor": 246.2,
    "flores_100_eng-lao": 298.3,
    "flores_100_eng-lav": 234.8,
    "flores_100_eng-lin": 213.3,
    "flores_100_eng-lit": 223.8,
    "flores_100_eng-ltz": 218,
    "flores_100_eng-lug": 215.1,
    "flores_100_eng-luo": 1145.5,
    "flores_100_eng-mal": 242,
    "flores_100_eng-mar": 279.5,
    "flores_100_eng-mkd": 209.5,
    "flores_100_eng-mlt": 235.9,
    "flores_100_eng-mon": 1164.8,
    "flores_100_eng-mri": 216.5,
    "flores_100_eng-msa": 217,
    "flores_100_eng-mya": 295.5,
    "flores_100_eng-nld": 199.5,
    "flores_100_eng-nob": 196.4,
    "flores_100_eng-npi": 283.6,
    "flores_100_eng-nso": 221.4,
    "flores_100_eng-nya": 220.6,
    "flores_100_eng-oci": 214.7,
    "flores_100_eng-orm": 1281.6,
    "flores_100_eng-ory": 304.8,
    "flores_100_eng-pan": 303.5,
    "flores_100_eng-pol": 204.3,
    "flores_100_eng-por": 184.2,
    "flores_100_eng-pus": 280,
    "flores_100_eng-ron": 202.4,
    "flores_100_eng-rus": 203.3,
    "flores_100_eng-slk": 206.2,
    "flores_100_eng-slv": 203.5,
    "flores_100_eng-sna": 226.8,
    "flores_100_eng-snd": 276.6,
    "flores_100_eng-som": 1280.8,
    "flores_100_eng-spa": 209.9,
    "flores_100_eng-srp": 206.2,
    "flores_100_eng-swe": 188,
    "flores_100_eng-swh": 229.9,
    "flores_100_eng-tam": 297.6,
    "flores_100_eng-tel": 294.1,
    "flores_100_eng-tgk": 244.5,
    "flores_100_eng-tgl": 227.9,
    "flores_100_eng-tha": 298,
    "flores_100_eng-tur": 1254.8,
    "flores_100_eng-ukr": 207.1,
    "flores_100_eng-umb": 232.8,
    "flores_100_eng-urd": 274.4,
    "flores_100_eng-uzb": 228.3,
    "flores_100_eng-vie": 265.3,
    "flores_100_eng-wol": 221.4,
    "flores_100_eng-xho": 225.8,
    "flores_100_eng-yor": 259.3,
    "flores_100_eng-zho_simpl": 192.5,
    "flores_100_eng-zho_trad": 212.3,
    "flores_100_eng-zul": 225,
    "flores_100_est-eng": 197.5,
    "flores_100_fas-eng": 271.2,
    "flores_100_fin-eng": 203.8,
    "flores_100_fra-eng": 177.5,
    "flores_100_ful-eng": 205.7,
    "flores_100_gle-eng": 220.6,
    "flores_100_glg-eng": 185.8,
    "flores_100_guj-eng": 266.8,
    "flores_100_hau-eng": 207.3,
    "flores_100_heb-eng": 246.3,
    "flores_100_hin-eng": 268,
    "flores_100_hrv-eng": 211.4,
    "flores_100_hun-eng": 210.7,
    "flores_100_hye-eng": 265.2,
    "flores_100_ibo-eng": 222.4,
    "flores_100_ind-eng": 199.7,
    "flores_100_isl-eng": 195.9,
    "flores_100_ita-eng": 178.6,
    "flores_100_jav-eng": 196.7,
    "flores_100_jpn-eng": 204.4,
    "flores_100_kam-eng": 210.7,
    "flores_100_kan-eng": 263,
    "flores_100_kat-eng": 263.4,
    "flores_100_kaz-eng": 215.5,
    "flores_100_kea-eng": 195.5,
    "flores_100_khm-eng": 269.8,
    "flores_100_kir-eng": 220.6,
    "flores_100_kor-eng": 240.4,
    "flores_100_lao-eng": 263.2,
    "flores_100_lav-eng": 202.8,
    "flores_100_lin-eng": 195.7,
    "flores_100_lit-eng": 208.9,
    "flores_100_ltz-eng": 195.2,
    "flores_100_lug-eng": 197.9,
    "flores_100_luo-eng": 199.6,
    "flores_100_mal-eng": 262.8,
    "flores_100_mar-eng": 253.9,
    "flores_100_mkd-eng": 220.3,
    "flores_100_mlt-eng": 201.3,
    "flores_100_mon-eng": 226.9,
    "flores_100_mri-eng": 204.9,
    "flores_100_msa-eng": 199.9,
    "flores_100_mya-eng": 273.6,
    "flores_100_nld-eng": 189.5,
    "flores_100_nob-eng": 189,
    "flores_100_npi-eng": 271.1,
    "flores_100_nso-eng": 217.5,
    "flores_100_nya-eng": 212.7,
    "flores_100_oci-eng": 192.6,
    "flores_100_orm-eng": 214.4,
    "flores_100_ory-eng": 263.3,
    "flores_100_pan-eng": 261.8,
    "flores_100_pol-eng": 189.7,
    "flores_100_por-eng": 181,
    "flores_100_pus-eng": 251.1,
    "flores_100_ron-eng": 200.3,
    "flores_100_rus-eng": 219,
    "flores_100_slk-eng": 197.6,
    "flores_100_slv-eng": 193.1,
    "flores_100_sna-eng": 213.1,
    "flores_100_snd-eng": 251.8,
    "flores_100_som-eng": 207.1,
    "flores_100_spa-eng": 187.2,
    "flores_100_srp-eng": 213.3,
    "flores_100_swe-eng": 179.5,
    "flores_100_swh-eng": 212.3,
    "flores_100_tam-eng": 259.1,
    "flores_100_tel-eng": 274.6,
    "flores_100_tgk-eng": 225,
    "flores_100_tgl-eng": 200.7,
    "flores_100_tha-eng": 267.8,
    "flores_100_tur-eng": 195.9,
    "flores_100_ukr-eng": 207.3,
    "flores_100_umb-eng": 194.4,
    "flores_100_urd-eng": 272.5,
    "flores_100_uzb-eng": 203.3,
    "flores_100_vie-eng": 226.7,
    "flores_100_wol-eng": 204.6,
    "flores_100_xho-eng": 205.6,
    "flores_100_yor-eng": 224.8,
    "flores_100_zho_simpl-eng": 204.1,
    "flores_100_zho_trad-eng": 204,
    "flores_100_zul-eng": 219.7,
    "GaokaoBench_2010-2013_English_MCQs": 101.5,
    "GaokaoBench_2010-2022_Biology_MCQs": 191.1,
    "GaokaoBench_2010-2022_Biology_Open-ended_Questions": 1110,
    "GaokaoBench_2010-2022_Chemistry_MCQs": 212.3,
    "GaokaoBench_2010-2022_Chemistry_Open-ended_Questions": 221.7,
    "GaokaoBench_2010-2022_Chinese_Lang_and_Usage_MCQs": 496.7,
    "GaokaoBench_2010-2022_Chinese_Language_Ancient_Poetry_Reading": 264.8,
    "GaokaoBench_2010-2022_Chinese_Language_Classical_Chinese_Reading": 315.3,
    "GaokaoBench_2010-2022_Chinese_Language_Famous_Passages_and_Sentences_Dictation": 366.4,
    "GaokaoBench_2010-2022_Chinese_Language_Language_and_Writing_Skills_Open-ended_Questions": 436.4,
    "GaokaoBench_2010-2022_Chinese_Language_Literary_Text_Reading": 449.8,
    "GaokaoBench_2010-2022_Chinese_Language_Practical_Text_Reading": 280.3,
    "GaokaoBench_2010-2022_Chinese_Modern_Lit": 314.2,
    "GaokaoBench_2010-2022_English_Fill_in_Blanks": 283,
    "GaokaoBench_2010-2022_English_Reading_Comp": 858.1,
    "GaokaoBench_2010-2022_Geography_MCQs": 277.3,
    "GaokaoBench_2010-2022_Geography_Open-ended_Questions": 322.6,
    "GaokaoBench_2010-2022_History_MCQs": 265.6,
    "GaokaoBench_2010-2022_History_Open-ended_Questions": 1514.9,
    "GaokaoBench_2010-2022_Math_I_Fill-in-the-Blank": 742.7,
    "GaokaoBench_2010-2022_Math_I_MCQs": 217.6,
    "GaokaoBench_2010-2022_Math_I_Open-ended_Questions": 1490.2,
    "GaokaoBench_2010-2022_Math_II_Fill-in-the-Blank": 860.7,
    "GaokaoBench_2010-2022_Math_II_MCQs": 201.5,
    "GaokaoBench_2010-2022_Math_II_Open-ended_Questions": 1463.2,
    "GaokaoBench_2010-2022_Physics_MCQs": 661.8,
    "GaokaoBench_2010-2022_Physics_Open-ended_Questions": 590,
    "GaokaoBench_2010-2022_Political_Science_MCQs": 379.5,
    "GaokaoBench_2010-2022_Political_Science_Open-ended_Questions": 768.2,
    "GaokaoBench_2012-2022_English_Cloze_Test": 216.6,
    "GaokaoBench_2012-2022_English_Language_Error_Correction": 235,
    "GaokaoBench_2014-2022_English_Language_Cloze_Passage": 206.4,
    "gsm8k": 7568.1,
    "hellaswag": 2498,
    "lambada": 740.6,
    "lcsts": 7203.3,
    "lukaemon_mmlu_abstract_algebra": 128.7,
    "lukaemon_mmlu_anatomy": 161.9,
    "lukaemon_mmlu_astronomy": 259,
    "lukaemon_mmlu_business_ethics": 178.7,
    "lukaemon_mmlu_clinical_knowledge": 291.2,
    "lukaemon_mmlu_college_biology": 206.9,
    "lukaemon_mmlu_college_chemistry": 172.3,
    "lukaemon_mmlu_college_computer_science": 233,
    "lukaemon_mmlu_college_mathematics": 173,
    "lukaemon_mmlu_college_medicine": 349.3,
    "lukaemon_mmlu_college_physics": 153,
    "lukaemon_mmlu_computer_security": 130.6,
    "lukaemon_mmlu_conceptual_physics": 195.5,
    "lukaemon_mmlu_econometrics": 196.4,
    "lukaemon_mmlu_electrical_engineering": 184.2,
    "lukaemon_mmlu_elementary_mathematics": 491.5,
    "lukaemon_mmlu_formal_logic": 221.8,
    "lukaemon_mmlu_global_facts": 133.1,
    "lukaemon_mmlu_high_school_biology": 407.9,
    "lukaemon_mmlu_high_school_chemistry": 302,
    "lukaemon_mmlu_high_school_computer_science": 258.4,
    "lukaemon_mmlu_high_school_european_history": 716.7,
    "lukaemon_mmlu_high_school_geography": 219.4,
    "lukaemon_mmlu_high_school_government_and_politics": 244,
    "lukaemon_mmlu_high_school_macroeconomics": 396.3,
    "lukaemon_mmlu_high_school_mathematics": 362.2,
    "lukaemon_mmlu_high_school_microeconomics": 257.6,
    "lukaemon_mmlu_high_school_physics": 242.3,
    "lukaemon_mmlu_high_school_psychology": 691.5,
    "lukaemon_mmlu_high_school_statistics": 436.1,
    "lukaemon_mmlu_high_school_us_history": 869.9,
    "lukaemon_mmlu_high_school_world_history": 902.7,
    "lukaemon_mmlu_human_aging": 198.1,
    "lukaemon_mmlu_human_sexuality": 157.5,
    "lukaemon_mmlu_international_law": 210.9,
    "lukaemon_mmlu_jurisprudence": 143,
    "lukaemon_mmlu_logical_fallacies": 207.5,
    "lukaemon_mmlu_machine_learning": 199.8,
    "lukaemon_mmlu_management": 106.1,
    "lukaemon_mmlu_marketing": 258.4,
    "lukaemon_mmlu_medical_genetics": 128.3,
    "lukaemon_mmlu_miscellaneous": 615.1,
    "lukaemon_mmlu_moral_disputes": 415.6,
    "lukaemon_mmlu_moral_scenarios": 1311.7,
    "lukaemon_mmlu_nutrition": 460,
    "lukaemon_mmlu_philosophy": 288.2,
    "lukaemon_mmlu_prehistory": 443.3,
    "lukaemon_mmlu_professional_accounting": 499.7,
    "lukaemon_mmlu_professional_law": 9308.8,
    "lukaemon_mmlu_professional_medicine": 809,
    "lukaemon_mmlu_professional_psychology": 840.6,
    "lukaemon_mmlu_public_relations": 159.9,
    "lukaemon_mmlu_security_studies": 751.1,
    "lukaemon_mmlu_sociology": 268,
    "lukaemon_mmlu_us_foreign_policy": 142.8,
    "lukaemon_mmlu_virology": 184.3,
    "lukaemon_mmlu_world_religions": 145.4,
    "math": 37479.6,
    "mbpp": 3773,
    "MultiRC": 1996.3,
    "nq": 1282.1,
    "nq_1shot": 1400.1,
    "nq_5shot": 1806,
    "ocnli": 584.6,
    "ocnli_fc-dev": 71.9,
    "ocnli_fc-test": 442.6,
    "openai_humaneval": 613.7,
    "openbookqa_fact": 143.6,
    "piqa": 233.4,
    "race-high": 5607.2,
    "race-middle": 1258.7,
    "ReCoRD": 6588.7,
    "RTE": 102.7,
    "siqa": 291.4,
    "story_cloze": 184.6,
    "strategyqa": 7645.5,
    "summedits": 7981.1,
    "TheoremQA": 4932.9,
    "tnews-dev": 715.3,
    "tnews-test": 603.2,
    "triviaqa": 3127.8,
    "triviaqa_1shot": 5212.4,
    "triviaqa_5shot": 4463.7,
    "tydiqa-goldp_arabic": 1483.8,
    "tydiqa-goldp_bengali": 281.5,
    "tydiqa-goldp_english": 441.8,
    "tydiqa-goldp_finnish": 944,
    "tydiqa-goldp_indonesian": 688.4,
    "tydiqa-goldp_japanese": 615.8,
    "tydiqa-goldp_korean": 446.9,
    "tydiqa-goldp_russian": 1036.8,
    "tydiqa-goldp_swahili": 451.1,
    "tydiqa-goldp_telugu": 1561.6,
    "tydiqa-goldp_thai": 1362.9,
    "WiC": 83,
    "winogrande": 98.5,
    "WSC": 49.1,
    "Xsum": 1597.4
}
